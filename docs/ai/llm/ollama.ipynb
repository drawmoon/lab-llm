{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebfd4c1",
   "metadata": {},
   "source": [
    "Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型（LLMs）的过程。它提供用户友好的界面和功能，使先进的人工智能技术变得易于获取且可定制。\n",
    "\n",
    "**安装 Ollama**\n",
    "\n",
    "```sh\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "如果你发现 Ollama 没有默认调用你的 GPU，或者你通过容器（如 Docker）运行 Ollama，你可以通过设置此变量来显式设置 GPU 的分配策略。\n",
    "\n",
    "操作步骤：\n",
    "- 设置环境变量：`export OLLAMA_NUM_GPU=-1`\n",
    "- 启动 Ollama：`ollama serve`\n",
    "\n",
    "> 可以通过 `ollama list` 查看可用模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf566e5e",
   "metadata": {},
   "source": [
    "要使用 Ollama，它需要作为后台服务与脚本并行运行。由于 Jupyter Notebook 设计为顺序执行代码块，这使得同时运行两个代码块变得困难。作为变通方案，我们将使用 Python 的 subprocess 创建服务，确保其不会阻塞任何单元格的执行。\n",
    "\n",
    "通过命令 `ollama serve` 可启动服务。\n",
    "\n",
    "`time.sleep(5)` 添加了延迟，确保 Ollama 服务启动完成后再下载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ae78ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2026-01-12T10:01:39.070Z level=INFO source=routes.go:1554 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2026-01-12T10:01:39.070Z level=INFO source=images.go:493 msg=\"total blobs: 7\"\n",
      "time=2026-01-12T10:01:39.070Z level=INFO source=images.go:500 msg=\"total unused blobs removed: 0\"\n",
      "time=2026-01-12T10:01:39.071Z level=INFO source=routes.go:1607 msg=\"Listening on 127.0.0.1:11434 (version 0.13.5)\"\n",
      "time=2026-01-12T10:01:39.071Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2026-01-12T10:01:39.072Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 45767\"\n",
      "time=2026-01-12T10:01:39.102Z level=INFO source=runner.go:106 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
      "time=2026-01-12T10:01:39.103Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 40077\"\n",
      "time=2026-01-12T10:01:39.134Z level=INFO source=types.go:60 msg=\"inference compute\" id=cpu library=cpu compute=\"\" name=cpu description=cpu libdirs=ollama driver=\"\" pci_id=\"\" type=\"\" total=\"15.6 GiB\" available=\"11.3 GiB\"\n",
      "time=2026-01-12T10:01:39.134Z level=INFO source=routes.go:1648 msg=\"entering low vram mode\" \"total vram\"=\"0 B\" threshold=\"20.0 GiB\"\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a048578",
   "metadata": {},
   "source": [
    "**拉取模型**\n",
    "\n",
    "使用 `ollama pull qwen3:0.6b` 下载 LLM 模型。\n",
    "\n",
    "其他模型请访问 https://ollama.com/library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed3f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2026/01/12 - 10:01:52 | 200 |      50.745µs |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l[GIN] 2026/01/12 - 10:01:53 | 200 |  962.297607ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 7f4030143c1c: 100% ▕██████████████████▏ 522 MB                         \u001b[K\n",
      "pulling ae370d884f10: 100% ▕██████████████████▏ 1.7 KB                         \u001b[K\n",
      "pulling d18a5cc71b84: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling cff3f395ef37: 100% ▕██████████████████▏  120 B                         \u001b[K\n",
      "pulling b0830f4ff6a0: 100% ▕██████████████████▏  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull qwen3:0.6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1dcd6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "\u001b[33m  WARNING: The script dotenv is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed python-dotenv-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Global dependencies required by this notebook:\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2af65e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07eed98",
   "metadata": {},
   "source": [
    "**下面是 ollama SDK 调用模型简单的例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee92938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, annotated-types, pydantic, ollama\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [ollama]2m3/5\u001b[0m [pydantic]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 ollama-0.6.1 pydantic-2.12.5 pydantic-core-2.41.5 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498dbd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2026-01-12T10:02:06.421Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "time=2026-01-12T10:02:06.495Z level=INFO source=server.go:245 msg=\"enabling flash attention\"\n",
      "time=2026-01-12T10:02:06.495Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /home/codespace/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa --port 41917\"\n",
      "time=2026-01-12T10:02:06.496Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"15.6 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
      "time=2026-01-12T10:02:06.496Z level=INFO source=server.go:746 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2026-01-12T10:02:06.507Z level=INFO source=runner.go:1405 msg=\"starting ollama engine\"\n",
      "time=2026-01-12T10:02:06.508Z level=INFO source=runner.go:1440 msg=\"Server listening on 127.0.0.1:41917\"\n",
      "time=2026-01-12T10:02:06.518Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:02:06.547Z level=INFO source=ggml.go:136 msg=\"\" architecture=qwen3 file_type=Q4_K_M name=\"Qwen3 0.6B\" description=\"\" num_tensors=311 num_key_values=29\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\n",
      "time=2026-01-12T10:02:06.554Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2026-01-12T10:02:06.565Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=ggml.go:482 msg=\"offloading 0 repeating layers to GPU\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=ggml.go:486 msg=\"offloading output layer to CPU\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=ggml.go:494 msg=\"offloaded 0/29 layers to GPU\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"492.8 MiB\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"30.0 MiB\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=device.go:272 msg=\"total memory\" size=\"970.8 MiB\"\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2026-01-12T10:02:06.693Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2026-01-12T10:02:06.696Z level=INFO source=server.go:1372 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2026-01-12T10:02:06.948Z level=INFO source=server.go:1376 msg=\"llama runner started in 0.45 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2026/01/12 - 10:02:16 | 200 | 10.066402388s |       127.0.0.1 | POST     \"/api/chat\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "机器学习是一种人工智能技术，旨在让计算机从数据中学习规律和模式，从而做出预测或决策。核心概念包括：\n",
       "\n",
       "1. **数据**：训练模型的基础，包括原始数据集和标注信息（如图片中的类别标签）。\n",
       "2. **算法**：用于处理数据并生成预测或决策的数学方法（如线性回归、决策树等）。\n",
       "3. **目标**：模型的最终目的是识别规律、预测结果或优化决策过程。\n",
       "4. **应用场景**：广泛应用于图像识别、推荐系统、自然语言处理等领域。\n",
       "\n",
       "简单来说，机器学习就像“用数据训练智能体”，让它从经验中“学习”如何解决问题。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "resp = client.chat(\n",
    "    model=\"qwen3:0.6b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"请解释一下机器学习的基本概念/no_think\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(resp[\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ffbc0b",
   "metadata": {},
   "source": [
    "**下面是使用 `langchain-openai` 调用模型简单的例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45464d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain[openai]\n",
      "  Downloading langchain-1.2.3-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain[openai])\n",
      "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain[openai])\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langchain[openai]) (2.12.5)\n",
      "Collecting langchain-openai (from langchain[openai])\n",
      "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading langsmith-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[openai]) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[openai]) (6.0.3)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain[openai]) (4.15.0)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain[openai]) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading langgraph_sdk-0.3.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /home/codespace/.local/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (0.28.1)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai])\n",
      "  Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[openai]) (2.32.5)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[openai])\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[openai]) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[openai]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain[openai]) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain[openai]) (1.3.1)\n",
      "Collecting openai<3.0.0,>=1.109.1 (from langchain-openai->langchain[openai])\n",
      "  Downloading openai-2.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai->langchain[openai])\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai])\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai])\n",
      "  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting tqdm>4 (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain[openai])\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Downloading langchain-1.2.3-py3-none-any.whl (106 kB)\n",
      "Downloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.2-py3-none-any.whl (66 kB)\n",
      "Downloading langsmith-0.6.2-py3-none-any.whl (282 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading uuid_utils-0.13.0-cp39-abi3-manylinux_2_24_x86_64.whl (342 kB)\n",
      "Downloading orjson-3.11.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
      "Downloading openai-2.15.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, tqdm, tenacity, regex, ormsgpack, orjson, jsonpatch, jiter, distro, tiktoken, requests-toolbelt, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain\n",
      "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/22\u001b[0m [tqdm]\u001b[33m  WARNING: The script tqdm is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/22\u001b[0m [orjson]\u001b[33m  WARNING: The script distro is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/22\u001b[0m [openai]\u001b[33m  WARNING: The script openai is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [langchain]22\u001b[0m [langgraph]openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 jiter-0.12.0 jsonpatch-1.33 langchain-1.2.3 langchain-core-1.2.7 langchain-openai-1.1.7 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.2 langsmith-0.6.2 openai-2.15.0 orjson-3.11.5 ormsgpack-1.12.1 regex-2025.11.3 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.12.0 tqdm-4.67.1 uuid-utils-0.13.0 xxhash-3.6.0 zstandard-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae62bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The length of the hypotenuse in a right-angled triangle can be determined using **Pythagoras' Theorem**, which states that in a right-angled triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides:  \n",
       "\n",
       "$$\n",
       "c^2 = a^2 + b^2\n",
       "$$  \n",
       "\n",
       "Where:  \n",
       "- $ c $ is the length of the hypotenuse,  \n",
       "- $ a $ and $ b $ are the lengths of the other two sides.  \n",
       "\n",
       "Since no specific values are provided, the length of the hypotenuse is expressed using this formula rather than numerical values.  \n",
       "\n",
       "**Final Answer:**  \n",
       "The length of the hypotenuse is given by the formula $ c^2 = a^2 + b^2 $.  \n",
       "\n",
       "For example, if $ a = 3 $ and $ b = 4 $, the hypotenuse $ c $ would be:  \n",
       "$$\n",
       "c = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5.\n",
       "$$  \n",
       "\n",
       "This means the final answer is left in the format $ c^2 $ as requested."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models.base import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"qwen3:0.6b\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "display(Markdown(chain.invoke({\"question\": \"What's the length of hypotenuse in a right angled triangle\"})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
