{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dde945a",
   "metadata": {},
   "source": [
    "### 环境准备\n",
    "由于需要模型，所以下面会安装 ollama 用于 langextract 模型调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0c3933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Global dependencies required by this notebook:\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1264527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c8306",
   "metadata": {},
   "source": [
    "要使用 Ollama，它需要作为后台服务与脚本并行运行。由于 Jupyter Notebook 设计为顺序执行代码块，这使得同时运行两个代码块变得困难。作为变通方案，我们将使用 Python 的 subprocess 创建服务，确保其不会阻塞任何单元格的执行。\n",
    "\n",
    "通过命令 `ollama serve` 可启动服务。\n",
    "\n",
    "`time.sleep(5)` 添加了延迟，确保 Ollama 服务启动完成后再下载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "271898af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2026-01-12T10:31:05.012Z level=INFO source=routes.go:1554 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false OLLAMA_VULKAN:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2026-01-12T10:31:05.013Z level=INFO source=images.go:493 msg=\"total blobs: 7\"\n",
      "time=2026-01-12T10:31:05.013Z level=INFO source=images.go:500 msg=\"total unused blobs removed: 0\"\n",
      "time=2026-01-12T10:31:05.013Z level=INFO source=routes.go:1607 msg=\"Listening on 127.0.0.1:11434 (version 0.13.5)\"\n",
      "time=2026-01-12T10:31:05.014Z level=INFO source=runner.go:67 msg=\"discovering available GPUs...\"\n",
      "time=2026-01-12T10:31:05.014Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 41547\"\n",
      "time=2026-01-12T10:31:05.043Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --port 41223\"\n",
      "time=2026-01-12T10:31:05.076Z level=INFO source=runner.go:106 msg=\"experimental Vulkan support disabled.  To enable, set OLLAMA_VULKAN=1\"\n",
      "time=2026-01-12T10:31:05.076Z level=INFO source=types.go:60 msg=\"inference compute\" id=cpu library=cpu compute=\"\" name=cpu description=cpu libdirs=ollama driver=\"\" pci_id=\"\" type=\"\" total=\"15.6 GiB\" available=\"10.1 GiB\"\n",
      "time=2026-01-12T10:31:05.076Z level=INFO source=routes.go:1648 msg=\"entering low vram mode\" \"total vram\"=\"0 B\" threshold=\"20.0 GiB\"\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3715e",
   "metadata": {},
   "source": [
    "**拉取模型**\n",
    "\n",
    "使用 `ollama pull qwen3:0.6b` 下载 LLM 模型。\n",
    "\n",
    "其他模型请访问 https://ollama.com/library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa04d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2026/01/12 - 10:30:03 | 200 |      77.905µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l[GIN] 2026/01/12 - 10:30:04 | 200 |  969.684449ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 7f4030143c1c: 100% ▕██████████████████▏ 522 MB                         \u001b[K\n",
      "pulling ae370d884f10: 100% ▕██████████████████▏ 1.7 KB                         \u001b[K\n",
      "pulling d18a5cc71b84: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling cff3f395ef37: 100% ▕██████████████████▏  120 B                         \u001b[K\n",
      "pulling b0830f4ff6a0: 100% ▕██████████████████▏  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull qwen3:0.6b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab4b05",
   "metadata": {},
   "source": [
    "### langextract 示例\n",
    "安装依赖："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4f9b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langextract[openai]\n",
      "  Downloading langextract-1.1.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting absl-py>=1.0.0 (from langextract[openai])\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiohttp>=3.8.0 (from langextract[openai])\n",
      "  Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting async_timeout>=4.0.0 (from langextract[openai])\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting exceptiongroup>=1.1.0 (from langextract[openai])\n",
      "  Downloading exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-genai>=1.39.0 (from langextract[openai])\n",
      "  Downloading google_genai-1.57.0-py3-none-any.whl.metadata (53 kB)\n",
      "Collecting google-cloud-storage>=2.14.0 (from langextract[openai])\n",
      "  Downloading google_cloud_storage-3.7.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting ml-collections>=0.1.0 (from langextract[openai])\n",
      "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting more-itertools>=8.0.0 (from langextract[openai])\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting numpy>=1.20.0 (from langextract[openai])\n",
      "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pandas>=1.3.0 (from langextract[openai])\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: pydantic>=1.8.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langextract[openai]) (2.12.5)\n",
      "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langextract[openai]) (1.2.1)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /home/codespace/.local/lib/python3.12/site-packages (from langextract[openai]) (6.0.3)\n",
      "Requirement already satisfied: regex>=2023.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langextract[openai]) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.25.0 in /home/codespace/.local/lib/python3.12/site-packages (from langextract[openai]) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langextract[openai]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from langextract[openai]) (4.15.0)\n",
      "Requirement already satisfied: openai>=1.50.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langextract[openai]) (2.15.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp>=3.8.0->langextract[openai]) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.8.0->langextract[openai])\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.8.0->langextract[openai]) (3.11)\n",
      "Collecting google-auth<3.0.0,>=2.26.1 (from google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting google-api-core<3.0.0,>=2.27.0 (from google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading google_api_core-2.29.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.2 (from google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading google_cloud_core-2.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.7.2 (from google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading google_resumable_media-2.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-crc32c<2.0.0,>=1.1.3 (from google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading google_crc32c-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading protobuf-6.33.3-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading proto_plus-1.27.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.25.0->langextract[openai]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.25.0->langextract[openai]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.25.0->langextract[openai]) (2025.11.12)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage>=2.14.0->langextract[openai])\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai>=1.39.0->langextract[openai]) (4.11.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai>=1.39.0->langextract[openai]) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-genai>=1.39.0->langextract[openai]) (9.1.2)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai>=1.39.0->langextract[openai])\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-genai>=1.39.0->langextract[openai]) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from google-genai>=1.39.0->langextract[openai]) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai>=1.39.0->langextract[openai]) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai>=1.39.0->langextract[openai]) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.8.0->langextract[openai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.8.0->langextract[openai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.8.0->langextract[openai]) (0.4.2)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai>=1.50.0->langextract[openai]) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->langextract[openai]) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.3.0->langextract[openai])\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->langextract[openai]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->langextract[openai]) (1.17.0)\n",
      "Downloading langextract-1.1.1-py3-none-any.whl (124 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading aiohttp-3.13.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading google_cloud_storage-3.7.0-py3-none-any.whl (303 kB)\n",
      "Downloading google_api_core-2.29.0-py3-none-any.whl (173 kB)\n",
      "Downloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
      "Downloading google_cloud_core-2.5.0-py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (33 kB)\n",
      "Downloading google_resumable_media-2.8.0-py3-none-any.whl (81 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading proto_plus-1.27.0-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.33.3-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_genai-1.57.0-py3-none-any.whl (713 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
      "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, websockets, pyasn1, protobuf, propcache, numpy, multidict, more-itertools, google-crc32c, frozenlist, exceptiongroup, async_timeout, aiohappyeyeballs, absl-py, yarl, rsa, pyasn1-modules, proto-plus, pandas, ml-collections, googleapis-common-protos, google-resumable-media, aiosignal, google-auth, aiohttp, google-api-core, google-genai, google-cloud-core, google-cloud-storage, langextract\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/30\u001b[0m [websockets]\u001b[33m  WARNING: The script websockets is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/30\u001b[0m [numpy]uf]\u001b[33m  WARNING: The scripts f2py and numpy-config are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/30\u001b[0m [yarl]e-crc32c]\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/30\u001b[0m [langextract]\u001b[0m [google-cloud-storage]a]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 async_timeout-5.0.1 exceptiongroup-1.3.1 frozenlist-1.8.0 google-api-core-2.29.0 google-auth-2.47.0 google-cloud-core-2.5.0 google-cloud-storage-3.7.0 google-crc32c-1.8.0 google-genai-1.57.0 google-resumable-media-2.8.0 googleapis-common-protos-1.72.0 langextract-1.1.1 ml-collections-1.1.0 more-itertools-10.8.0 multidict-6.7.0 numpy-2.4.1 pandas-2.3.3 propcache-0.4.1 proto-plus-1.27.0 protobuf-6.33.3 pyasn1-0.6.1 pyasn1-modules-0.4.2 pytz-2025.2 rsa-4.9.1 websockets-15.0.1 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langextract[openai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f45e7f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/langextract/__init__.py:55: UserWarning: 'use_schema_constraints' is ignored when 'model' is provided. The model should already be configured with schema constraints.\n",
      "  return extract_func(*args, **kwargs)\n",
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mqwen3:0.6b\u001b[0m, current=\u001b[92m68\u001b[0m chars, processed=\u001b[92m0\u001b[0m chars:  [00:00]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2026-01-12T10:31:14.655Z level=WARN source=cpu_linux.go:130 msg=\"failed to parse CPU allowed micro secs\" error=\"strconv.ParseInt: parsing \\\"max\\\": invalid syntax\"\n",
      "time=2026-01-12T10:31:14.734Z level=INFO source=server.go:245 msg=\"enabling flash attention\"\n",
      "time=2026-01-12T10:31:14.734Z level=INFO source=server.go:429 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model /home/codespace/.ollama/models/blobs/sha256-7f4030143c1c477224c5434f8272c662a8b042079a0a584f0a27a1684fe2e1fa --port 35701\"\n",
      "time=2026-01-12T10:31:14.735Z level=INFO source=sched.go:443 msg=\"system memory\" total=\"15.6 GiB\" free=\"10.1 GiB\" free_swap=\"0 B\"\n",
      "time=2026-01-12T10:31:14.735Z level=INFO source=server.go:746 msg=\"loading model\" \"model layers\"=29 requested=-1\n",
      "time=2026-01-12T10:31:14.748Z level=INFO source=runner.go:1405 msg=\"starting ollama engine\"\n",
      "time=2026-01-12T10:31:14.748Z level=INFO source=runner.go:1440 msg=\"Server listening on 127.0.0.1:35701\"\n",
      "time=2026-01-12T10:31:14.757Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:31:14.787Z level=INFO source=ggml.go:136 msg=\"\" architecture=qwen3 file_type=Q4_K_M name=\"Qwen3 0.6B\" description=\"\" num_tensors=311 num_key_values=29\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so\n",
      "time=2026-01-12T10:31:14.794Z level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)\n",
      "time=2026-01-12T10:31:14.809Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=runner.go:1278 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:Enabled KvSize:4096 KvCacheType: NumThreads:2 GPULayers:[] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=device.go:245 msg=\"model weights\" device=CPU size=\"492.8 MiB\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=device.go:256 msg=\"kv cache\" device=CPU size=\"448.0 MiB\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=device.go:267 msg=\"compute graph\" device=CPU size=\"30.0 MiB\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=device.go:272 msg=\"total memory\" size=\"970.8 MiB\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=sched.go:517 msg=\"loaded runners\" count=1\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=server.go:1338 msg=\"waiting for llama runner to start responding\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=ggml.go:482 msg=\"offloading 0 repeating layers to GPU\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=ggml.go:486 msg=\"offloading output layer to CPU\"\n",
      "time=2026-01-12T10:31:14.941Z level=INFO source=ggml.go:494 msg=\"offloaded 0/29 layers to GPU\"\n",
      "time=2026-01-12T10:31:14.951Z level=INFO source=server.go:1372 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2026-01-12T10:31:15.202Z level=INFO source=server.go:1376 msg=\"llama runner started in 0.47 seconds\"\n",
      "\u001b[94m\u001b[1mLangExtract\u001b[0m: model=\u001b[92mqwen3:0.6b\u001b[0m, current=\u001b[92m68\u001b[0m chars, processed=\u001b[92m0\u001b[0m chars:  [00:46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2026/01/12 - 10:32:00 | 200 |  46.40237364s |       127.0.0.1 | POST     \"/v1/chat/completions\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnotatedDocument(extractions=[Extraction(extraction_class='character', extraction_text='Lady Juliet', char_interval=CharInterval(start_pos=0, end_pos=11), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=1, group_index=0, description=None, attributes={'emotional_state': 'longingly'}), Extraction(extraction_class='emotion', extraction_text='gazed longingly', char_interval=CharInterval(start_pos=12, end_pos=27), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=2, group_index=1, description=None, attributes={'feeling': 'longing'}), Extraction(extraction_class='relationship', extraction_text='her heart aching for Romeo', char_interval=CharInterval(start_pos=42, end_pos=68), alignment_status=<AlignmentStatus.MATCH_EXACT: 'match_exact'>, extraction_index=3, group_index=2, description=None, attributes={'type': 'metaphor'})], text='Lady Juliet gazed longingly at the stars, her heart aching for Romeo')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langextract as lx\n",
    "import textwrap\n",
    "\n",
    "# 手动指定模型和提供者\n",
    "config = lx.factory.ModelConfig(\n",
    "    model_id=\"qwen3:0.6b\",\n",
    "    provider=\"OpenAILanguageModel\",\n",
    "    provider_kwargs={\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": os.getenv(\"API_KEY\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "model = lx.factory.create_model(config)\n",
    "\n",
    "# 1. 定义提示词和提取规则\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "    Extract characters, emotions, and relationships in order of appearance.\n",
    "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
    "    Provide meaningful attributes for each entity to add context.\"\"\")\n",
    "\n",
    "# 2. 提供示例来指导模型\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.\",\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"character\",\n",
    "                extraction_text=\"ROMEO\",\n",
    "                attributes={\"emotional_state\": \"wonder\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"emotion\",\n",
    "                extraction_text=\"But soft!\",\n",
    "                attributes={\"feeling\": \"gentle awe\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"relationship\",\n",
    "                extraction_text=\"Juliet is the sun\",\n",
    "                attributes={\"type\": \"metaphor\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the extraction\n",
    "lx.extract(\n",
    "    text_or_documents=\"Lady Juliet gazed longingly at the stars, her heart aching for Romeo\",\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model=model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
