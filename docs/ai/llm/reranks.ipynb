{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f889c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting mermaid-python\n",
      "  Downloading mermaid_python-0.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: ipython in /home/codespace/.local/lib/python3.12/site-packages (from mermaid-python) (9.2.0)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from ipython->mermaid-python) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython->mermaid-python) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/codespace/.local/lib/python3.12/site-packages (from jedi>=0.16->ipython->mermaid-python) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.12/site-packages (from pexpect>4.3->ipython->mermaid-python) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->mermaid-python) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->mermaid-python) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/codespace/.local/lib/python3.12/site-packages (from stack_data->ipython->mermaid-python) (0.2.3)\n",
      "Downloading mermaid_python-0.1-py3-none-any.whl (3.2 kB)\n",
      "Installing collected packages: mermaid-python\n",
      "Successfully installed mermaid-python-0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Global dependencies required by this notebook:\n",
    "# - python-dotenv: load environment variables like COHERE_API_KEY (install with `pip install python-dotenv`)\n",
    "# These are environment-level installs and should be run once per environment.\n",
    "%pip install python-dotenv\n",
    "%pip install mermaid-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617a2b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from mermaid import Mermaid\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe770569",
   "metadata": {},
   "source": [
    "### RRF Rerank\n",
    "\n",
    "RRF（Reciprocal Rank Fusion） 是一种 无模型、无训练的排序融合算法，核心思想是“一个结果在多个排序中都靠前，就更重要”。\n",
    "\n",
    "> 专业项目中常将 **RRF 作为召回融合层** → 再用 **Cross-Encoder / LLM Reranker** 做精排。\n",
    "\n",
    "**算法公式**\n",
    "\n",
    "$$ \\text{Score}(d) = \\sum_{i=1}^{n} \\frac{1}{k + \\text{rank}_i(d)} $$\n",
    "\n",
    "这里的 `k` 是平滑参数（默认 60，可调），用来控制前 `n` 名项的贡献分布。\n",
    "\n",
    "**原理**\n",
    "\n",
    "假设两个路径的 Top-5:\n",
    "\n",
    "|id|sparse rank|dense rank|\n",
    "|---|---|---|\n",
    "|101|1|2|\n",
    "|198|4|1|\n",
    "|175|5|4|\n",
    "|203|2|—|\n",
    "|150|3|—|\n",
    "\n",
    "RRF 融合后按综合得分排序，交叉出现的更高一致性者胜出。\n",
    "    \n",
    "**RRF Rerank 优点**\n",
    "\n",
    "- 无模型，纯工程级算法实现\n",
    "- 代码实现简单\n",
    "- 可以对不同检索器召回结果进行融合\n",
    "- 延迟极低"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811faace",
   "metadata": {},
   "source": [
    "**典型流程示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab4e3926",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-fe9a4cc5-6eb3-4996-bed1-64e5bba6d699\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = 'flowchart LR\\n    A[Query] --> B[BM25]\\n    A --> C[Embeddings]\\n    B --> D[RRF Rank]\\n    C --> D[RRF Rank]\\n    D --> E[Cross-Encoder]\\n';\n",
       "            const element = document.querySelector('.mermaid-fe9a4cc5-6eb3-4996-bed1-64e5bba6d699');\n",
       "            const { svg } = await mermaid.render('graphDiv-fe9a4cc5-6eb3-4996-bed1-64e5bba6d699', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x7af597be97c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(\"\"\"\n",
    "flowchart LR\n",
    "    A[Query] --> B[BM25]\n",
    "    A --> C[Embeddings]\n",
    "    B --> D[RRF Rank]\n",
    "    C --> D[RRF Rank]\n",
    "    D --> E[Cross-Encoder]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5942a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def rrf_rerank(documnents: list[list[str]], k: int = 60, limit: int | None = None) -> list[tuple[str, float]]:\n",
    "    \"\"\"Reciprocal Rank Fusion (RRF)\"\"\"\n",
    "    scores = defaultdict(float)\n",
    "    for ranked in documnents:\n",
    "        for rank, doc_id in enumerate(ranked, start=1):\n",
    "            scores[doc_id] += 1.0 / (k + rank)\n",
    "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:limit] if limit else ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1f114",
   "metadata": {},
   "source": [
    "**使用示例**\n",
    "\n",
    "假设我们已经用 BM25、BGE、Qwen 得到了召回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae0a1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [\"docA\", \"docB\", \"docC\", \"docD\"],  # BM25 排名结果\n",
    "    [\"docB\", \"docE\", \"docA\", \"docF\"],  # BGE Embedding 排名结果\n",
    "    [\"docC\", \"docA\", \"docG\", \"docH\"],  # Qwen Embedding 排名结果\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b92cb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRF result (default k=60):\n",
      "  Doc ID: docA, Score: 0.0484\n",
      "  Doc ID: docB, Score: 0.0325\n",
      "  Doc ID: docC, Score: 0.0323\n",
      "  Doc ID: docE, Score: 0.0161\n",
      "  Doc ID: docG, Score: 0.0159\n",
      "  Doc ID: docD, Score: 0.0156\n",
      "  Doc ID: docF, Score: 0.0156\n",
      "  Doc ID: docH, Score: 0.0156\n",
      "RRF result (k=20, limit=3):\n",
      "  Doc ID: docA, Score: 0.1366\n",
      "  Doc ID: docB, Score: 0.0931\n",
      "  Doc ID: docC, Score: 0.0911\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 使用默认的 k=60\n",
    "# ============================================================================\n",
    "result = rrf_rerank(data)\n",
    "print(\"RRF result (default k=60):\")\n",
    "for doc_id, score in result:\n",
    "    print(f\"  Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "# ============================================================================\n",
    "# 指定 k 值和 limit\n",
    "# ============================================================================\n",
    "result = rrf_rerank(data, k=20, limit=3)\n",
    "print(\"RRF result (k=20, limit=3):\")\n",
    "for doc_id, score in result:\n",
    "    print(f\"  Doc ID: {doc_id}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefa1a3",
   "metadata": {},
   "source": [
    "### WeightedRanker\n",
    "\n",
    "WeightedRanker 是一种加权融合排序器，对多个打分来源按权重加权求和，得到最终排序，常见于推荐系统、搜索排序融合。\n",
    "\n",
    "**算法公式**\n",
    "\n",
    "$$ Score(d) = \\sum_{i=1}^{n} w_i \\cdot score_i(d) $$\n",
    "\n",
    "- $d$：文档 / 候选结果\n",
    "- $score_i(d)$：第 $i$ 个模型或通道给 $d$ 的分数\n",
    "- $w_i$：该通道的权重\n",
    "- $n$：通道数量\n",
    "\n",
    "**WeightedRanker vs RRF**\n",
    "\n",
    "| 维度      | WeightedRanker | RRF     |\n",
    "| ------- | -------------- | ------- |\n",
    "| 是否用原始分数 |   用            |   只看名次  |\n",
    "| 是否需要归一化 |   必须           |   不需要   |\n",
    "| 是否可控    | 强        | 弱    |\n",
    "| 抗异常能力   | 一般             | 强 |\n",
    "| 工程复杂度   | 中              | 低       |\n",
    "\n",
    "在 BI 领域业务中，WeightedRanker 比 RRF 更适合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f5ea23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rank(docs_scores: dict[str, dict[str, float]], weights: dict[str, float], alpha_tie_break: bool = False):\n",
    "    final_scores = {}\n",
    "\n",
    "    for doc, scores in docs_scores.items():\n",
    "        # Calculate the weighted sum of scores for the current document\n",
    "        # Using a generator expression with sum() for conciseness and readability\n",
    "        # weights.get(k, 0) handles cases where a score type might not have a corresponding weight, defaulting to 0.\n",
    "        total = sum(weights.get(k, 0) * v for k, v in scores.items())\n",
    "        final_scores[doc] = total\n",
    "\n",
    "    # Define the sorting key based on the alpha_tie_break parameter\n",
    "    if alpha_tie_break:\n",
    "        # Sort primarily by score (descending), then by document name (ascending) for tie-breaking\n",
    "        # Multiplying score by -1 ensures descending order for scores when sorting ascending by the tuple.\n",
    "        sort_key = lambda x: (-x[1], x[0])\n",
    "        # Since the key is (-score, doc_name), sorted will naturally sort scores descending and doc_names ascending for ties.\n",
    "        return sorted(final_scores.items(), key=sort_key)\n",
    "    else:\n",
    "        # Sort only by score (descending), relying on Python's stable sort for ties if alpha_tie_break is False.\n",
    "        return sorted(final_scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e0c5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running tests for updated weighted_rank function ---\n",
      "\n",
      "Case 1: Basic functionality with multiple documents and scores\n",
      "[('doc_B', 0.74), ('doc_A', 0.72), ('doc_C', 0.7)]\n",
      "\n",
      "Case 2: Cases where some weights are zero\n",
      "[('doc_B', 0.9), ('doc_A', 0.8)]\n",
      "\n",
      "Case 3: Cases with missing keys in the weights dictionary (default weight should be 0)\n",
      "[('doc_B', 0.9), ('doc_A', 0.8)]\n",
      "\n",
      "Case 4: Edge case - empty docs_scores dictionary\n",
      "[]\n",
      "\n",
      "Case 5: Edge case - empty weights dictionary (all scores should have 0 weight)\n",
      "[('doc_A', 0.0), ('doc_B', 0.0)]\n",
      "\n",
      "Case 6: Scenarios where all scores for a document sum to zero or are equal\n",
      "[('doc_Z', 1.0), ('doc_X', 0.0), ('doc_Y', 0.0)]\n",
      "\n",
      "Case 7: Negative weights\n",
      "[('doc_A', 0.5), ('doc_B', -0.5)]\n",
      "\n",
      "Case 8: All scores equal, check descending order and stable sort\n",
      "[('doc_1', 1.0), ('doc_2', 1.0), ('doc_3', 1.0)]\n",
      "\n",
      "Case 9: All scores equal with alpha_tie_break = True\n",
      "[('doc_A', 1.0), ('doc_M', 1.0), ('doc_Z', 1.0)]\n",
      "\n",
      "Case 10: Mixed scores with alpha_tie_break = True, demonstrating tie-breaking\n",
      "[('doc_C', 0.9), ('doc_A', 0.7), ('doc_B', 0.7)]\n",
      "\n",
      "All updated test cases passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running tests for updated weighted_rank function ---\")\n",
    "\n",
    "print(\"\\nCase 1: Basic functionality with multiple documents and scores\")\n",
    "docs_scores_1 = {\n",
    "    'doc_A': {'score1': 0.8, 'score2': 0.6},\n",
    "    'doc_B': {'score1': 0.9, 'score2': 0.5},\n",
    "    'doc_C': {'score1': 0.7, 'score2': 0.7}\n",
    "}\n",
    "weights_1 = {'score1': 0.6, 'score2': 0.4}\n",
    "expected_output_1 = [('doc_B', 0.74), ('doc_A', 0.72), ('doc_C', 0.7)]\n",
    "actual_output_1 = weighted_rank(docs_scores_1, weights_1)\n",
    "assert actual_output_1 == expected_output_1, f\"Test Case 1 Failed: Expected {expected_output_1}, Got {actual_output_1}\"\n",
    "print(actual_output_1)\n",
    "\n",
    "print(\"\\nCase 2: Cases where some weights are zero\")\n",
    "docs_scores_2 = {\n",
    "    'doc_A': {'score1': 0.8, 'score2': 0.6},\n",
    "    'doc_B': {'score1': 0.9, 'score2': 0.5}\n",
    "}\n",
    "weights_2 = {'score1': 1.0, 'score2': 0.0}\n",
    "expected_output_2 = [('doc_B', 0.9), ('doc_A', 0.8)]\n",
    "actual_output_2 = weighted_rank(docs_scores_2, weights_2)\n",
    "assert actual_output_2 == expected_output_2, f\"Test Case 2 Failed: Expected {expected_output_2}, Got {actual_output_2}\"\n",
    "print(actual_output_2)\n",
    "\n",
    "print(\"\\nCase 3: Cases with missing keys in the weights dictionary (default weight should be 0)\")\n",
    "docs_scores_3 = {\n",
    "    'doc_A': {'score1': 0.8, 'score2': 0.6},\n",
    "    'doc_B': {'score1': 0.9, 'score2': 0.5}\n",
    "}\n",
    "weights_3 = {'score1': 1.0}\n",
    "expected_output_3 = [('doc_B', 0.9), ('doc_A', 0.8)] # score2 should have 0 weight\n",
    "actual_output_3 = weighted_rank(docs_scores_3, weights_3)\n",
    "assert actual_output_3 == expected_output_3, f\"Test Case 3 Failed: Expected {expected_output_3}, Got {actual_output_3}\"\n",
    "print(actual_output_3)\n",
    "\n",
    "print(\"\\nCase 4: Edge case - empty docs_scores dictionary\")\n",
    "docs_scores_4 = {}\n",
    "weights_4 = {'score1': 0.5, 'score2': 0.5}\n",
    "expected_output_4 = []\n",
    "actual_output_4 = weighted_rank(docs_scores_4, weights_4)\n",
    "assert actual_output_4 == expected_output_4, f\"Test Case 4 Failed: Expected {expected_output_4}, Got {actual_output_4}\"\n",
    "print(actual_output_4)\n",
    "\n",
    "print(\"\\nCase 5: Edge case - empty weights dictionary (all scores should have 0 weight)\")\n",
    "docs_scores_5 = {\n",
    "    'doc_A': {'score1': 0.8, 'score2': 0.6},\n",
    "    'doc_B': {'score1': 0.9, 'score2': 0.5}\n",
    "}\n",
    "weights_5 = {}\n",
    "actual_output_5 = weighted_rank(docs_scores_5, weights_5)\n",
    "for doc, score in actual_output_5:\n",
    "    assert score == 0.0, f\"Test Case 5 Failed: Score for {doc} should be 0.0, Got {score}\"\n",
    "assert set([doc for doc, _ in actual_output_5]) == set(docs_scores_5.keys()), f\"Test Case 5 Failed: Documents mismatch\"\n",
    "print(actual_output_5)\n",
    "\n",
    "print(\"\\nCase 6: Scenarios where all scores for a document sum to zero or are equal\")\n",
    "docs_scores_6 = {\n",
    "    'doc_X': {'s1': 0.5, 's2': -0.5},\n",
    "    'doc_Y': {'s1': 0.0, 's2': 0.0},\n",
    "    'doc_Z': {'s1': 1.0, 's2': 0.0}\n",
    "}\n",
    "weights_6 = {'s1': 1.0, 's2': 1.0}\n",
    "expected_output_6 = [('doc_Z', 1.0), ('doc_X', 0.0), ('doc_Y', 0.0)]\n",
    "actual_output_6 = weighted_rank(docs_scores_6, weights_6)\n",
    "assert round(actual_output_6[0][1], 5) == expected_output_6[0][1], f\"Test Case 6 Failed doc_Z score: Expected {expected_output_6[0][1]}, Got {actual_output_6[0][1]}\"\n",
    "assert round(actual_output_6[1][1], 5) == expected_output_6[1][1], f\"Test Case 6 Failed doc_X score: Expected {expected_output_6[1][1]}, Got {actual_output_6[1][1]}\"\n",
    "assert round(actual_output_6[2][1], 5) == expected_output_6[2][1], f\"Test Case 6 Failed doc_Y score: Expected {expected_output_6[2][1]}, Got {actual_output_6[2][1]}\"\n",
    "assert set([doc for doc, _ in actual_output_6]) == set(docs_scores_6.keys())\n",
    "print(actual_output_6)\n",
    "\n",
    "print(\"\\nCase 7: Negative weights\")\n",
    "docs_scores_7 = {\n",
    "    'doc_A': {'s1': 1.0, 's2': 0.5},\n",
    "    'doc_B': {'s1': 0.5, 's2': 1.0}\n",
    "}\n",
    "weights_7 = {'s1': 1.0, 's2': -1.0}\n",
    "expected_output_7 = [('doc_A', 0.5), ('doc_B', -0.5)]\n",
    "actual_output_7 = weighted_rank(docs_scores_7, weights_7)\n",
    "assert actual_output_7 == expected_output_7, f\"Test Case 7 Failed: Expected {expected_output_7}, Got {actual_output_7}\"\n",
    "print(actual_output_7)\n",
    "\n",
    "print(\"\\nCase 8: All scores equal, check descending order and stable sort\")\n",
    "docs_scores_8 = {\n",
    "    'doc_1': {'s': 1.0},\n",
    "    'doc_2': {'s': 1.0},\n",
    "    'doc_3': {'s': 1.0}\n",
    "}\n",
    "weights_8 = {'s': 1.0}\n",
    "expected_output_8 = [('doc_1', 1.0), ('doc_2', 1.0), ('doc_3', 1.0)] # relies on stable sort\n",
    "actual_output_8 = weighted_rank(docs_scores_8, weights_8)\n",
    "assert actual_output_8 == expected_output_8, f\"Test Case 8 Failed: Expected {expected_output_8}, Got {actual_output_8}\"\n",
    "print(actual_output_8)\n",
    "\n",
    "print(\"\\nCase 9: All scores equal with alpha_tie_break = True\")\n",
    "docs_scores_9 = {\n",
    "    'doc_Z': {'s': 1.0},\n",
    "    'doc_A': {'s': 1.0},\n",
    "    'doc_M': {'s': 1.0}\n",
    "}\n",
    "weights_9 = {'s': 1.0}\n",
    "expected_output_9 = [('doc_A', 1.0), ('doc_M', 1.0), ('doc_Z', 1.0)] # Alphabetical tie-breaking\n",
    "actual_output_9 = weighted_rank(docs_scores_9, weights_9, alpha_tie_break=True)\n",
    "assert actual_output_9 == expected_output_9, f\"Test Case 9 Failed (alpha_tie_break): Expected {expected_output_9}, Got {actual_output_9}\"\n",
    "print(actual_output_9)\n",
    "\n",
    "print(\"\\nCase 10: Mixed scores with alpha_tie_break = True, demonstrating tie-breaking\")\n",
    "docs_scores_10 = {\n",
    "    'doc_B': {'score': 0.7},\n",
    "    'doc_A': {'score': 0.7},\n",
    "    'doc_C': {'score': 0.9}\n",
    "}\n",
    "weights_10 = {'score': 1.0}\n",
    "# Expected: doc_C first, then doc_A, then doc_B due to alphabetical tie-breaking\n",
    "expected_output_10 = [('doc_C', 0.9), ('doc_A', 0.7), ('doc_B', 0.7)]\n",
    "actual_output_10 = weighted_rank(docs_scores_10, weights_10, alpha_tie_break=True)\n",
    "assert actual_output_10 == expected_output_10, f\"Test Case 10 Failed (alpha_tie_break mixed scores): Expected {expected_output_10}, Got {actual_output_10}\"\n",
    "print(actual_output_10)\n",
    "\n",
    "print(\"\\nAll updated test cases passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a023b25",
   "metadata": {},
   "source": [
    "WeightedRanker 需要将不同通道的分数映射到同一尺度（通常是 0～1），才能发挥权重意义。\n",
    "\n",
    "下面是常见的归一化算法介绍与代码实现\n",
    "\n",
    "**Min-Max Scaling**\n",
    "\n",
    "This technique rescales a feature to a fixed range, usually 0 to 1. It is useful when you need to normalize data to a specific boundary.\n",
    "\n",
    "$$\n",
    "\\hat{s}_i = \\frac{s_i - \\min(s)}{\\max(s) - \\min(s)}\n",
    "$$\n",
    "\n",
    "**适用场景：**\n",
    "\n",
    "- 分数区间稳定\n",
    "- 没有太多离群点\n",
    "\n",
    "**Z-score Normalization (Standardization)**\n",
    "\n",
    "This technique rescales data to have a mean of 0 and a standard deviation of 1. It is useful when you have features with different scales and distributions, and it is less affected by outliers than Min-Max scaling.\n",
    "\n",
    "$$\n",
    "\\hat{s}_i = \\frac{s_i - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**适用场景：**\n",
    "\n",
    "- 消除均值/方差影响\n",
    "- 分数分布接近正态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a79041ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaling and Z-score normalization functions defined.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def min_max_scale(scores: dict[str, float]) -> dict[str, float]:\n",
    "    \"\"\"Applies Min-Max scaling to a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        scores (dict[str, float]): A dictionary where keys are score names (or doc IDs)\n",
    "                                   and values are the scores to be scaled.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: A new dictionary with the Min-Max scaled scores.\n",
    "    \"\"\"\n",
    "    if not scores:\n",
    "        return {}\n",
    "\n",
    "    values = list(scores.values())\n",
    "    min_val = min(values)\n",
    "    max_val = max(values)\n",
    "\n",
    "    scaled_scores = {}\n",
    "    if max_val == min_val:\n",
    "        # Avoid division by zero, all scores become 0.5 (mid-range) or 0 if min_val is 0\n",
    "        # If all values are the same, they all map to 0.5 (or 0 if range is 0) in [0,1]\n",
    "        for k, v in scores.items():\n",
    "            scaled_scores[k] = 0.5 if v != 0 else 0.0 # Handles case like all scores are 0, should stay 0.\n",
    "                                                      # Otherwise, if all are same non-zero, map to 0.5\n",
    "    else:\n",
    "        for k, v in scores.items():\n",
    "            scaled_scores[k] = (v - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_scores\n",
    "\n",
    "def z_score_normalize(scores: dict[str, float]) -> dict[str, float]:\n",
    "    \"\"\"Applies Z-score normalization (standardization) to a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        scores (dict[str, float]): A dictionary where keys are score names (or doc IDs)\n",
    "                                   and values are the scores to be normalized.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: A new dictionary with the Z-score normalized scores.\n",
    "    \"\"\"\n",
    "    if not scores:\n",
    "        return {}\n",
    "\n",
    "    values = list(scores.values())\n",
    "    n = len(values)\n",
    "\n",
    "    if n == 0:\n",
    "        return {}\n",
    "\n",
    "    mean = sum(values) / n\n",
    "    # Calculate sample standard deviation (unbiased estimate)\n",
    "    std_dev = math.sqrt(sum((x - mean) ** 2 for x in values) / (n - 1)) if n > 1 else 0.0\n",
    "\n",
    "    normalized_scores = {}\n",
    "    if std_dev == 0:\n",
    "        # If standard deviation is zero, all values are the same. Normalize to 0.\n",
    "        for k, v in scores.items():\n",
    "            normalized_scores[k] = 0.0\n",
    "    else:\n",
    "        for k, v in scores.items():\n",
    "            normalized_scores[k] = (v - mean) / std_dev\n",
    "\n",
    "    return normalized_scores\n",
    "\n",
    "print(\"Min-Max scaling and Z-score normalization functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89cdae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Normalization Function Tests ---\n",
      "\n",
      "--- Min-Max Scaling Tests ---\n",
      "\n",
      "1: Basic Min-Max scaling (0 to 1 range)\n",
      "{'s1': 0.0, 's2': 0.5, 's3': 1.0}\n",
      "\n",
      "2: Min-Max scaling with negative values\n",
      "{'s1': 0.0, 's2': 0.5, 's3': 1.0}\n",
      "\n",
      "3: Min-Max scaling with all identical values\n",
      "{'s1': 0.5, 's2': 0.5, 's3': 0.5}\n",
      "\n",
      "4: Min-Max scaling with empty input\n",
      "{}\n",
      "\n",
      "5: Min-Max scaling with a single value\n",
      "{'s1': 0.5}\n",
      "\n",
      "6: Min-Max scaling with all zero values\n",
      "{'s1': 0.0, 's2': 0.0, 's3': 0.0}\n",
      "\n",
      "--- Z-score Normalization Tests ---\n",
      "\n",
      "7: Basic Z-score normalization\n",
      "{'s1': -1.2649110640673518, 's2': -0.6324555320336759, 's3': 0.0, 's4': 0.6324555320336759, 's5': 1.2649110640673518}\n",
      "\n",
      "8: Z-score normalization with negative values\n",
      "{'s1': -1.2649110640673518, 's2': -0.6324555320336759, 's3': 0.0, 's4': 0.6324555320336759, 's5': 1.2649110640673518}\n",
      "\n",
      "9: Z-score normalization with all identical values (std_dev = 0)\n",
      "{'s1': 0.0, 's2': 0.0, 's3': 0.0}\n",
      "\n",
      "10: Z-score normalization with empty input\n",
      "{}\n",
      "\n",
      "11: Z-score normalization with a single value\n",
      "{'s1': 0.0}\n",
      "\n",
      "All normalization function tests passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Running Normalization Function Tests ---\")\n",
    "\n",
    "# --- Min-Max Scaling Tests ---\n",
    "print(\"\\n--- Min-Max Scaling Tests ---\")\n",
    "\n",
    "print(\"\\n1: Basic Min-Max scaling (0 to 1 range)\")\n",
    "scores_mm_1 = {'s1': 10.0, 's2': 20.0, 's3': 30.0}\n",
    "expected_mm_1 = {'s1': 0.0, 's2': 0.5, 's3': 1.0}\n",
    "actual_mm_1 = min_max_scale(scores_mm_1)\n",
    "assert all(abs(actual_mm_1[k] - expected_mm_1[k]) < 1e-9 for k in expected_mm_1), f\"Min-Max Test 1 Failed: Expected {expected_mm_1}, Got {actual_mm_1}\"\n",
    "print(actual_mm_1)\n",
    "\n",
    "print(\"\\n2: Min-Max scaling with negative values\")\n",
    "scores_mm_2 = {'s1': -5.0, 's2': 0.0, 's3': 5.0}\n",
    "expected_mm_2 = {'s1': 0.0, 's2': 0.5, 's3': 1.0}\n",
    "actual_mm_2 = min_max_scale(scores_mm_2)\n",
    "assert all(abs(actual_mm_2[k] - expected_mm_2[k]) < 1e-9 for k in expected_mm_2), f\"Min-Max Test 2 Failed: Expected {expected_mm_2}, Got {actual_mm_2}\"\n",
    "print(actual_mm_2)\n",
    "\n",
    "print(\"\\n3: Min-Max scaling with all identical values\")\n",
    "scores_mm_3 = {'s1': 7.0, 's2': 7.0, 's3': 7.0}\n",
    "expected_mm_3 = {'s1': 0.5, 's2': 0.5, 's3': 0.5}\n",
    "actual_mm_3 = min_max_scale(scores_mm_3)\n",
    "assert all(abs(actual_mm_3[k] - expected_mm_3[k]) < 1e-9 for k in expected_mm_3), f\"Min-Max Test 3 Failed: Expected {expected_mm_3}, Got {actual_mm_3}\"\n",
    "print(actual_mm_3)\n",
    "\n",
    "print(\"\\n4: Min-Max scaling with empty input\")\n",
    "scores_mm_4 = {}\n",
    "expected_mm_4 = {}\n",
    "actual_mm_4 = min_max_scale(scores_mm_4)\n",
    "assert actual_mm_4 == expected_mm_4, f\"Min-Max Test 4 Failed: Expected {expected_mm_4}, Got {actual_mm_4}\"\n",
    "print(actual_mm_4)\n",
    "\n",
    "print(\"\\n5: Min-Max scaling with a single value\")\n",
    "scores_mm_5 = {'s1': 100.0}\n",
    "expected_mm_5 = {'s1': 0.5}\n",
    "actual_mm_5 = min_max_scale(scores_mm_5)\n",
    "assert all(abs(actual_mm_5[k] - expected_mm_5[k]) < 1e-9 for k in expected_mm_5), f\"Min-Max Test 5 Failed: Expected {expected_mm_5}, Got {actual_mm_5}\"\n",
    "print(actual_mm_5)\n",
    "\n",
    "print(\"\\n6: Min-Max scaling with all zero values\")\n",
    "scores_mm_6 = {'s1': 0.0, 's2': 0.0, 's3': 0.0}\n",
    "expected_mm_6 = {'s1': 0.0, 's2': 0.0, 's3': 0.0}\n",
    "actual_mm_6 = min_max_scale(scores_mm_6)\n",
    "assert all(abs(actual_mm_6[k] - expected_mm_6[k]) < 1e-9 for k in expected_mm_6), f\"Min-Max Test 6 Failed: Expected {expected_mm_6}, Got {actual_mm_6}\"\n",
    "print(actual_mm_6)\n",
    "\n",
    "# --- Z-score Normalization Tests ---\n",
    "print(\"\\n--- Z-score Normalization Tests ---\")\n",
    "\n",
    "print(\"\\n7: Basic Z-score normalization\")\n",
    "scores_zs_1 = {'s1': 1.0, 's2': 2.0, 's3': 3.0, 's4': 4.0, 's5': 5.0}\n",
    "# Mean = 3.0, Std Dev = 1.58113883\n",
    "expected_zs_1 = {\n",
    "    's1': (1.0 - 3.0) / 1.58113883,\n",
    "    's2': (2.0 - 3.0) / 1.58113883,\n",
    "    's3': (3.0 - 3.0) / 1.58113883,\n",
    "    's4': (4.0 - 3.0) / 1.58113883,\n",
    "    's5': (5.0 - 3.0) / 1.58113883\n",
    "}\n",
    "actual_zs_1 = z_score_normalize(scores_zs_1)\n",
    "assert all(abs(actual_zs_1[k] - expected_zs_1[k]) < 1e-9 for k in expected_zs_1), f\"Z-score Test 7 Failed: Expected {expected_zs_1}, Got {actual_zs_1}\"\n",
    "print(actual_zs_1)\n",
    "\n",
    "print(\"\\n8: Z-score normalization with negative values\")\n",
    "scores_zs_2 = {'s1': -2.0, 's2': -1.0, 's3': 0.0, 's4': 1.0, 's5': 2.0}\n",
    "# Mean = 0.0, Std Dev = 1.58113883\n",
    "expected_zs_2 = {\n",
    "    's1': (-2.0 - 0.0) / 1.58113883,\n",
    "    's2': (-1.0 - 0.0) / 1.58113883,\n",
    "    's3': (0.0 - 0.0) / 1.58113883,\n",
    "    's4': (1.0 - 0.0) / 1.58113883,\n",
    "    's5': (2.0 - 0.0) / 1.58113883\n",
    "}\n",
    "actual_zs_2 = z_score_normalize(scores_zs_2)\n",
    "assert all(abs(actual_zs_2[k] - expected_zs_2[k]) < 1e-9 for k in expected_zs_2), f\"Z-score Test 8 Failed: Expected {expected_zs_2}, Got {actual_zs_2}\"\n",
    "print(actual_zs_2)\n",
    "\n",
    "print(\"\\n9: Z-score normalization with all identical values (std_dev = 0)\")\n",
    "scores_zs_3 = {'s1': 5.0, 's2': 5.0, 's3': 5.0}\n",
    "expected_zs_3 = {'s1': 0.0, 's2': 0.0, 's3': 0.0}\n",
    "actual_zs_3 = z_score_normalize(scores_zs_3)\n",
    "assert all(abs(actual_zs_3[k] - expected_zs_3[k]) < 1e-9 for k in expected_zs_3), f\"Z-score Test 9 Failed: Expected {expected_zs_3}, Got {actual_zs_3}\"\n",
    "print(actual_zs_3)\n",
    "\n",
    "print(\"\\n10: Z-score normalization with empty input\")\n",
    "scores_zs_4 = {}\n",
    "expected_zs_4 = {}\n",
    "actual_zs_4 = z_score_normalize(scores_zs_4)\n",
    "assert actual_zs_4 == expected_zs_4, f\"Z-score Test 10 Failed: Expected {expected_zs_4}, Got {actual_zs_4}\"\n",
    "print(actual_zs_4)\n",
    "\n",
    "print(\"\\n11: Z-score normalization with a single value\")\n",
    "scores_zs_5 = {'s1': 100.0}\n",
    "expected_zs_5 = {'s1': 0.0}\n",
    "actual_zs_5 = z_score_normalize(scores_zs_5)\n",
    "assert all(abs(actual_zs_5[k] - expected_zs_5[k]) < 1e-9 for k in expected_zs_5), f\"Z-score Test 11 Failed: Expected {expected_zs_5}, Got {actual_zs_5}\"\n",
    "print(actual_zs_5)\n",
    "\n",
    "print(\"\\nAll normalization function tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffae8fd",
   "metadata": {},
   "source": [
    "### BGE Rerank\n",
    "\n",
    "BGE Rerank 是 BAAI（智源）发布的重排序模型，属于 Cross-Encoder Reranker。\n",
    "\n",
    "**`m3` = Multi-lingual + Multi-task + Multi-granularity**\n",
    "- Multi-lingual：中英双强\n",
    "- Multi-task：检索、问答、排序\n",
    "- Multi-granularity：短文本 / 长文档\n",
    "\n",
    "**BGE Rerank 优势**\n",
    "\n",
    "- 可本地部署\n",
    "- 可离线\n",
    "- 开源\n",
    "- 可二次微调\n",
    "- 多语言（尤其中文非常强）\n",
    "\n",
    "**模型定位**\n",
    "\n",
    "- 面向 RAG / 搜索 / 问答\n",
    "- 用于 Top-N → Top-K 精排\n",
    "- 替代商业 Rerank API（如 Cohere）\n",
    "\n",
    "**模型结构**\n",
    "\n",
    "- Cross-Encoder\n",
    "- Transformer（BERT 类）\n",
    "- 输入：`[CLS] Query [SEP] Document [SEP]`\n",
    "- 输出：`relevance_score ∈ ℝ`\n",
    "\n",
    "**典型应用案例**\n",
    "\n",
    "1. BM25 / 向量 / RRF 召回 Top-N（20~200）\n",
    "2. 使用 `bge-rerank-v2-m3` 批量打分\n",
    "3. 排序，取 Top-K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4151c",
   "metadata": {},
   "source": [
    "### CohereRerank\n",
    "\n",
    "Cohere Rerank 是一种 基于大模型 Cross-Encoder 的重排序服务。它对查询、候选文档进行逐对语义理解打分，然后重新排序。\n",
    "\n",
    "**Cohere 优点**\n",
    "\n",
    "- 无需部署，API 调用，基于 Token 收费\n",
    "- 语义理解能力，对自然语言、问句、业务表达非常友好\n",
    "- 对复杂、长 Query 表现好\n",
    "- 能纠正 embedding / BM25 的误召回"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499f9dd",
   "metadata": {},
   "source": [
    "步骤一: 安装 cohere SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be8cc670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cohere in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.20.1)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /home/codespace/.local/lib/python3.12/site-packages (from cohere) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (2.12.5)\n",
      "Requirement already satisfied: pydantic-core>=2.18.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (2.41.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (0.22.2)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (2.32.4.20260107)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from cohere) (4.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (2025.4.26)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere) (1.2.4)\n",
      "Requirement already satisfied: filelock in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
      "Requirement already satisfied: shellingham in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
      "Requirement already satisfied: typer-slim in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.21.1)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.12/site-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the Cohere library (run this in a cell first)\n",
    "%pip install cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399a70c",
   "metadata": {},
   "source": [
    "步骤二：初始化 Cohere 客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "\n",
    "# Initialize the Cohere client with your API key\n",
    "# Replace with your actual API key from the dashboard\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "co = cohere.ClientV2(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce01a165",
   "metadata": {},
   "source": [
    "示例 1: 英文文档排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084ccdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is artificial intelligence?\n",
      "\n",
      "Reranked Results:\n",
      "--------------------------------------------------\n",
      "Rank: 3\n",
      "Document: Machine learning is a subset of artificial intelligence\n",
      "Relevance Score: 0.2788\n",
      "--------------------------------------------------\n",
      "Rank: 1\n",
      "Document: Python is a programming language\n",
      "Relevance Score: 0.0001\n",
      "--------------------------------------------------\n",
      "Rank: 4\n",
      "Document: Dogs are popular pets\n",
      "Relevance Score: 0.0001\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example documents to rerank\n",
    "documents = [\n",
    "    \"Python is a programming language\",\n",
    "    \"The Eiffel Tower is in Paris\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Dogs are popular pets\",\n",
    "    \"Coffee is made from roasted beans\",\n",
    "]\n",
    "\n",
    "# Query you want to rerank documents for\n",
    "query = \"What is artificial intelligence?\"\n",
    "\n",
    "# Call the rerank endpoint\n",
    "results = co.rerank(\n",
    "    model=\"rerank-english-v3.0\",  # or \"rerank-multilingual-v3.0\" for multiple languages\n",
    "    query=query,\n",
    "    documents=documents,\n",
    "    top_n=3  # Return top 3 most relevant documents\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Reranked Results:\")\n",
    "print(\"-\" * 50)\n",
    "for result in results.results:\n",
    "    print(f\"Rank: {result.index + 1}\")\n",
    "    print(f\"Document: {documents[result.index]}\")\n",
    "    print(f\"Relevance Score: {result.relevance_score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f53be3",
   "metadata": {},
   "source": [
    "示例2: 中文文档排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bfec6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 机器学习模型\n",
      "\n",
      "Reranked Results:\n",
      "--------------------------------------------------\n",
      "Rank: 4\n",
      "Document: 机器学习算法从数据中学习模式\n",
      "Relevance Score: 0.0680\n",
      "--------------------------------------------------\n",
      "Rank: 2\n",
      "Document: 深度学习是机器学习的一个分支，使用神经网络技术\n",
      "Relevance Score: 0.0378\n",
      "--------------------------------------------------\n",
      "Rank: 7\n",
      "Document: Python广泛用于机器学习\n",
      "Relevance Score: 0.0042\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chinese_documents = [\n",
    "    \"狗是许多家庭中受欢迎的宠物\",\n",
    "    \"深度学习是机器学习的一个分支，使用神经网络技术\",\n",
    "    \"法国的首都是巴黎\",\n",
    "    \"机器学习算法从数据中学习模式\",\n",
    "    \"猫和狗都是常见的动物\",\n",
    "    \"人工智能正在革新技术产业\",\n",
    "    \"Python广泛用于机器学习\",\n",
    "]\n",
    "\n",
    "chinese_query = \"机器学习模型\"\n",
    "\n",
    "chinese_results = co.rerank(\n",
    "    model=\"rerank-multilingual-v3.0\",\n",
    "    query=chinese_query,\n",
    "    documents=chinese_documents,\n",
    "    top_n=3\n",
    ")\n",
    "\n",
    "print(f\"Query: {chinese_query}\\n\")\n",
    "print(\"Reranked Results:\")\n",
    "print(\"-\" * 50)\n",
    "for result in chinese_results.results:\n",
    "    print(f\"Rank: {result.index + 1}\")\n",
    "    print(f\"Document: {chinese_documents[result.index]}\")\n",
    "    print(f\"Relevance Score: {result.relevance_score:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c039c",
   "metadata": {},
   "source": [
    "### ColBERT\n",
    "\n",
    "ColBERT 是由斯坦福大学未来数据系统实验室开发的一个最先进的神经网络搜索系统，在多个顶级学术会议上发表论文（SIGIR'20、TACL'21、NeurIPS'21 等）。\n",
    "\n",
    "**核心特点**\n",
    "\n",
    "- 快速准确的检索：能在毫秒级时间内对大规模文本集合进行基于 BERT 的搜索\n",
    "- 后期交互机制：关键创新是\"晚期交互\"（late interaction）- 将查询和文段编码成 token 级别的嵌入矩阵，而不是单个向量，实现更细粒度的相似度匹配\n",
    "- 高效扩展：使用 MaxSim 操作符进行可扩展的向量相似性计算，能在保持质量的同时处理大规模语料库\n",
    "\n",
    "**工作流程**\n",
    "\n",
    "1. 预处理：将数据转换为 TSV 格式（查询和文段）\n",
    "2. 下载模型：获取在 MS MARCO 数据集上预训练的 ColBERT 检查点\n",
    "3. 索引：对所有文段进行编码和索引以实现快速检索\n",
    "4. 搜索：使用模型和索引检索每个查询的 Top-k 文段\n",
    "\n",
    "**应用场景**\n",
    "\n",
    "该项目适合需要高效语义搜索的应用，如信息检索、问答系统、知识库搜索等。它在准确性和速度之间取得了很好的平衡。\n",
    "\n",
    "**示例代码**\n",
    "\n",
    "安装依赖\n",
    "\n",
    "```bash\n",
    "pip install colbert-ai[torch,faiss-gpu]\n",
    "```\n",
    "\n",
    "下面对 ColBERT 做一个简单的使用示例，ColBERT 提供 [Colab 在线版本 Notebook](https://colab.research.google.com/github/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb)\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# 基础检索示例\n",
    "# ============================================================================\n",
    "from colbert.data import Queries\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Searcher\n",
    "\n",
    "with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
    "    config = ColBERTConfig(root=\"/path/to/experiments\")\n",
    "    \n",
    "    # 创建搜索器（使用预构建的索引）\n",
    "    searcher = Searcher(index=\"msmarco.nbits=2\", config=config)\n",
    "    \n",
    "    # 加载查询\n",
    "    queries = Queries(\"/path/to/MSMARCO/queries.dev.small.tsv\")\n",
    "    \n",
    "    # 执行搜索（检索 Top-100）\n",
    "    ranking = searcher.search_all(queries, k=100)\n",
    "    \n",
    "    # 保存结果\n",
    "    ranking.save(\"msmarco.nbits=2.ranking.tsv\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# 索引构建示例\n",
    "# ============================================================================\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Indexer\n",
    "\n",
    "with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n",
    "    config = ColBERTConfig(nbits=2, root=\"/path/to/experiments\")\n",
    "    \n",
    "    # 创建索引器\n",
    "    indexer = Indexer(checkpoint=\"/path/to/checkpoint\", config=config)\n",
    "    \n",
    "    # 对集合进行索引\n",
    "    indexer.index(\n",
    "        name=\"msmarco.nbits=2\",\n",
    "        collection=\"/path/to/MSMARCO/collection.tsv\"\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
